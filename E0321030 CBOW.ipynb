{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_LuemwWWiaif"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "D4Z5ua81iiBI"
   },
   "outputs": [],
   "source": [
    "path = \"shakespeare.txt\"\n",
    "with open(path, 'r') as file:\n",
    "     data: str = file.read()\n",
    "\n",
    "data = re.sub(r'[,!?;-]', '.',data)\n",
    "data = word_tokenize(data)\n",
    "data = [token.lower() for token in data if token.isalpha() or token == '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aHY4X2m4iiDp",
    "outputId": "553f20c5-a240-41f9-d946-559e8eea8198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of tokens = 1084877 \n",
      " ['this', 'is', 'the', 'etext', 'file', 'presented', 'by', 'project', 'gutenberg', '.', 'and', 'is', 'presented', 'in', 'cooperation', 'with', 'world', 'library', '.', 'inc']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Number of tokens = {len(data)} \\n {data[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1XAshraiiGg",
    "outputId": "e1f27dc0-ea67-4e43-c9cc-f87c59200dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  22892\n",
      "Most frequent tokens:  [('.', 197374), ('the', 27597), ('and', 26724), ('i', 22059), ('to', 19188), ('of', 18180), ('a', 14654), ('you', 13828), ('my', 12465), ('that', 11503), ('is', 10995), ('in', 10990), ('not', 9481), ('for', 8241), ('with', 7994), ('me', 7769), ('it', 7718), ('be', 7081), ('your', 6873), ('his', 6851)]\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(\"Size of vocabulary: \",len(fdist) )\n",
    "print(\"Most frequent tokens: \",fdist.most_common(20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hrBPLhliiIw",
    "outputId": "1bd1ee11-c342-4790-df54-02582458ebfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  22892\n"
     ]
    }
   ],
   "source": [
    "def get_dict(data: list[str]) -> tuple:\n",
    "    words = sorted(list(set(data)))\n",
    "    n = len(words)\n",
    "    idx = 0\n",
    "    word2Ind = {}\n",
    "    Ind2word = {}\n",
    "    for k in words:\n",
    "        word2Ind[k] = idx\n",
    "        Ind2word[idx] = k\n",
    "        idx += 1\n",
    "    return word2Ind, Ind2word\n",
    "\n",
    "\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UMYe3BCiiLI",
    "outputId": "f55138c4-debe-46ef-d842-a589a56e0a6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'king' :   11116\n",
      "Word which has index 2743:   butcherly\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
    "print(\"Word which has index 2743:  \",Ind2word[2743] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "OOLePqXukyfl"
   },
   "outputs": [],
   "source": [
    "def init_parameters(N: int, V: int, random_seed: int= 1) -> dict:\n",
    "    np.random.seed(random_seed)\n",
    "    parameters = {}\n",
    "\n",
    "    parameters[\"W1\"] = np.random.rand(N, V)\n",
    "    parameters[\"b1\"] = np.zeros(shape=(N, 1))\n",
    "\n",
    "    parameters[\"W2\"] = np.random.rand(V, N)\n",
    "    parameters[\"b2\"] = np.zeros(shape=(V, 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Boqm3F9lk0uv",
    "outputId": "96780bf8-912f-49fe-9665-fff11daf3219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape: (4, 10)\n",
      "W2 shape: (10, 4)\n",
      "b1 shape: (4, 1)\n",
      "b2 shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "tmp_N = 4\n",
    "tmp_V = 10\n",
    "params = init_parameters(tmp_N,tmp_V)\n",
    "assert params['W1'].shape == ((tmp_N,tmp_V))\n",
    "assert params['W2'].shape == ((tmp_V,tmp_N))\n",
    "print(f\"W1 shape: {params['W1'].shape}\")\n",
    "print(f\"W2 shape: {params['W2'].shape}\")\n",
    "print(f\"b1 shape: {params['b1'].shape}\")\n",
    "print(f\"b2 shape: {params['b2'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "AUmkxLsck2iv"
   },
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "def softmax(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
    "    y = np.exp(z) / np.sum(np.exp(z), 0, keepdims=True)\n",
    "\n",
    "    return y\n",
    "\n",
    "def sigmoid(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def relu(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
    "    return np.maximum(z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Qv4ztpluk5A2",
    "outputId": "282548af-ea61-4cc6-8b08-c25b6d4dd270"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.73105858, 0.88079708],\n",
       "       [0.5       , 0.26894142, 0.11920292]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = np.array([[1,2,3],\n",
    "                [1,1,1]\n",
    "               ])\n",
    "tmp_sm = softmax(tmp)\n",
    "display(tmp_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "_nj5N8Bxk6dQ"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(x: np.ndarray, params: dict) -> tuple:\n",
    "    z1 = np.dot(params['W1'], x) + params['b1']\n",
    "    h = relu(z1)\n",
    "\n",
    "    z2 = np.dot(params['W2'], h) + params['b2']\n",
    "    y_hat = z2\n",
    "\n",
    "    return y_hat, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqYRu2mfk8ZW",
    "outputId": "702a5f11-626a-4899-950f-d84f66b95d21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call forward_prop\n",
      "\n",
      "z has shape (10, 1)\n",
      "z has values:\n",
      "[[1.82887324]\n",
      " [1.38466287]\n",
      " [0.60100484]\n",
      " [0.83284144]\n",
      " [1.37936774]\n",
      " [1.20420827]\n",
      " [1.26273995]\n",
      " [2.0149547 ]\n",
      " [1.10825153]\n",
      " [1.93910889]]\n"
     ]
    }
   ],
   "source": [
    "tmp_x = np.array([[0,1,0,0,0,0,0,0,0,0]]).T\n",
    "tmp_z, tmp_h = forward_propagation(tmp_x, params)\n",
    "print(\"call forward_prop\")\n",
    "print()\n",
    "# Look at output\n",
    "print(f\"z has shape {tmp_z.shape}\")\n",
    "print(\"z has values:\")\n",
    "print(tmp_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "clqTV7vmk-9u"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_hat: np.ndarray, y: np.ndarray,\n",
    "                       batch_size: int) -> Union[float, List[float]]:\n",
    "    logprobs = np.multiply(np.log(y_hat), y)\n",
    "    cost = -1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "yNNCZy26lAh-"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(x: np.ndarray, y: np.ndarray, y_hat: np.ndarray, h: np.ndarray,\n",
    "                         params: dict, batch_size: int) -> dict:\n",
    "    grads_params = {}\n",
    "\n",
    "    l1 = np.dot(params['W2'].T ,(y_hat - y))\n",
    "\n",
    "    grads_params['W1'] = np.dot(l1, x.T) / batch_size\n",
    "    grads_params['W2'] = np.dot((y_hat - y), h.T) / batch_size\n",
    "\n",
    "    grads_params['b1'] = np.sum(l1, axis=1, keepdims= True) / batch_size\n",
    "    grads_params['b2'] = np.sum((y_hat - y), axis= 1, keepdims= True) / batch_size\n",
    "\n",
    "    return grads_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "p4NMb14wlDg2"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_idx(words, word2Ind):\n",
    "    idx = []\n",
    "    for word in words:\n",
    "        idx = idx + [word2Ind[word]]\n",
    "    return idx\n",
    "\n",
    "def pack_idx_with_frequency(context_words, word2Ind):\n",
    "    freq_dict = defaultdict(int)\n",
    "    for word in context_words:\n",
    "        freq_dict[word] += 1\n",
    "    idxs = get_idx(context_words, word2Ind)\n",
    "    packed = []\n",
    "    for i in range(len(idxs)):\n",
    "        idx = idxs[i]\n",
    "        freq = freq_dict[context_words[i]]\n",
    "        packed.append((idx, freq))\n",
    "    return packed\n",
    "\n",
    "def get_vectors(data, word2Ind, V, C):\n",
    "    i = C\n",
    "    while True:\n",
    "        y = np.zeros(V)\n",
    "        x = np.zeros(V)\n",
    "        center_word = data[i]\n",
    "        y[word2Ind[center_word]] = 1\n",
    "        context_words = data[(i - C) : i] + data[(i + 1) : (i + C + 1)]\n",
    "        num_ctx_words = len(context_words)\n",
    "        for idx, freq in pack_idx_with_frequency(context_words, word2Ind):\n",
    "            x[idx] = freq / num_ctx_words\n",
    "        yield x, y\n",
    "        i += 1\n",
    "        if i >= len(data) - C:\n",
    "            print(\"i is being set to\", C)\n",
    "            i = C\n",
    "\n",
    "def get_batches(data, word2Ind, V, C, batch_size):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for x, y in get_vectors(data, word2Ind, V, C):\n",
    "        if len(batch_x) < batch_size:\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "        else:\n",
    "            yield np.array(batch_x).T, np.array(batch_y).T\n",
    "            batch_x = []\n",
    "            batch_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "OqV3MK0hlGyO"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(data: list[str], word2Ind: dict, N: int,\n",
    "                     V: int, epochs: int, alpha: float= 0.03,\n",
    "                     random_seed: int= 282) -> dict:\n",
    "\n",
    "    param = init_parameters(N,V, random_seed=random_seed)\n",
    "\n",
    "    batch_size = 128\n",
    "    epoch = 0\n",
    "    C = 2\n",
    "\n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "\n",
    "        z, h = forward_propagation(x, param)\n",
    "        yhat = softmax(z)\n",
    "\n",
    "        cost = cross_entropy_loss(yhat, y, batch_size)\n",
    "        if ( (epoch+1) % 10 == 0):\n",
    "            print(f\"iters: {epoch + 1} cost: {cost:.6f}\")\n",
    "\n",
    "\n",
    "        grads = backward_propagation(x,y, yhat, h, param, batch_size)\n",
    "\n",
    "\n",
    "        param['W1'] = param['W1'] - alpha * grads['W1']\n",
    "        param['W2'] = param['W2'] - alpha * grads['W2']\n",
    "        param['b1'] = param['b1'] - alpha * grads['b1']\n",
    "        param['b2'] = param['b2'] - alpha * grads['b2']\n",
    "\n",
    "        epoch +=1\n",
    "        if epoch == epochs:\n",
    "            break\n",
    "        if epoch % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ms-xxduXlJEO",
    "outputId": "91b0e008-73ae-4200-9e91-b62c07979658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 cost: 10.515535\n",
      "iters: 20 cost: 10.707387\n",
      "iters: 30 cost: 10.320237\n",
      "iters: 40 cost: 10.211154\n",
      "iters: 50 cost: 10.123117\n",
      "iters: 60 cost: 10.107501\n",
      "iters: 70 cost: 9.980620\n",
      "iters: 80 cost: 9.905205\n",
      "iters: 90 cost: 9.822018\n",
      "iters: 100 cost: 9.705264\n",
      "iters: 110 cost: 9.742620\n",
      "iters: 120 cost: 9.485970\n",
      "iters: 130 cost: 9.366885\n",
      "iters: 140 cost: 9.579672\n",
      "iters: 150 cost: 9.518378\n"
     ]
    }
   ],
   "source": [
    "C = 2\n",
    "N = 50\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 150\n",
    "print(\"Call gradient_descent\")\n",
    "params = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "wizDBhPjlK4A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
